{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested-Cross Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redman/mambaforge/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_27357/1310781227.py:106: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========logP=========\n",
      "R^2 = 0.858\n",
      "MAE =  0.15\n",
      "MSE =  0.031\n",
      "=========logP=========\n",
      "R^2 = 0.798\n",
      "MAE =  0.178\n",
      "MSE =  0.063\n"
     ]
    }
   ],
   "source": [
    "# manual nested cross-validation for random forest on a classification dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#Data download\n",
    "train = pd.read_csv('/home/redman/blackthorn/enamine/f/data/train_logP_v4_features_2.2.csv')\n",
    "train = train.drop(['fold_id'], axis=1)\n",
    "train = train.dropna(axis=0)\n",
    "test = pd.read_csv('/home/redman/blackthorn/enamine/f/data/test_logP_v4_features_2.2.csv')\n",
    "test = test.dropna(axis=0)\n",
    "cv_inner = StratifiedKFoldContinuous(n_splits=2, shuffle=True, random_state=1)\n",
    "X_train = train.drop(['logP'], axis=1)\n",
    "y_train = train['logP']\n",
    "X_test = test.drop(['logP'], axis=1)\n",
    "y_test = test['logP']\n",
    "#define the model\n",
    "model = ElasticNet(random_state=1)\n",
    "# define search space\n",
    "param_grid = {\n",
    "                'alpha'     : [0.1,1,10,0.01],\n",
    "                'l1_ratio'  : np.arange(0.40,1.00,0.10),\n",
    "                'tol'       : [0.0001,0.001]\n",
    "            }\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, param_grid, scoring=['r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error'], cv=cv_inner, refit='r2', return_train_score=True)\n",
    "# execute search\n",
    "result = search.fit(X_train, y_train)\n",
    "# get the best performing model fit on the whole training set\n",
    "best_model = result.best_estimator_\n",
    "# evaluate model on the hold out dataset\n",
    "yhat = best_model.predict(X_test)\n",
    "# evaluate the model\n",
    "def metrics(test, feature, regr=best_model):\n",
    "    y_pred = regr.predict(test.drop([feature], axis=1))\n",
    "    r2 = r2_score(y_pred=y_pred, y_true = test[feature])\n",
    "    print(f'=========%s========='%(feature))\n",
    "    print('R^2 = '+str(round(r2, 3)))\n",
    "    print('MAE = ', round(mae(y_true = test[feature], y_pred=y_pred), 3))\n",
    "    print('MSE = ', round(mse(y_true = test[feature], y_pred=y_pred), 3))\n",
    "metrics(test, 'logP')\n",
    "metrics(train, 'logP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58776168, 0.5869133 , 0.56956858, 0.56866354, 0.54475645,\n",
       "       0.54380843, 0.52856372, 0.52708934, 0.52203393, 0.52091415,\n",
       "       0.5191257 , 0.51796504, 0.51810282, 0.51772172, 0.52243634,\n",
       "       0.52231281, 0.52483844, 0.52486331, 0.52630363, 0.52633979,\n",
       "       0.52715509, 0.52714213, 0.5277299 , 0.52772084, 0.3886759 ,\n",
       "       0.38875796, 0.3260629 , 0.32615315, 0.27954997, 0.27955867,\n",
       "       0.25001265, 0.25001904, 0.21183987, 0.21184468, 0.16685822,\n",
       "       0.16685818, 0.52719798, 0.5273059 , 0.52149305, 0.52169414,\n",
       "       0.51968528, 0.52007   , 0.51111664, 0.51133001, 0.51089772,\n",
       "       0.51117299, 0.51764063, 0.51783681])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.cv_results_['mean_test_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(result.cv_results_['mean_test_neg_root_mean_squared_error']== (max(result.cv_results_['mean_test_neg_root_mean_squared_error'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_alpha', 'param_l1_ratio', 'param_tol', 'params', 'split0_test_r2', 'split1_test_r2', 'mean_test_r2', 'std_test_r2', 'rank_test_r2', 'split0_train_r2', 'split1_train_r2', 'mean_train_r2', 'std_train_r2', 'split0_test_neg_mean_absolute_error', 'split1_test_neg_mean_absolute_error', 'mean_test_neg_mean_absolute_error', 'std_test_neg_mean_absolute_error', 'rank_test_neg_mean_absolute_error', 'split0_train_neg_mean_absolute_error', 'split1_train_neg_mean_absolute_error', 'mean_train_neg_mean_absolute_error', 'std_train_neg_mean_absolute_error', 'split0_test_neg_root_mean_squared_error', 'split1_test_neg_root_mean_squared_error', 'mean_test_neg_root_mean_squared_error', 'std_test_neg_root_mean_squared_error', 'rank_test_neg_root_mean_squared_error', 'split0_train_neg_root_mean_squared_error', 'split1_train_neg_root_mean_squared_error', 'mean_train_neg_root_mean_squared_error', 'std_train_neg_root_mean_squared_error'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_array, column_or_1d\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "class StratifiedKFoldContinuous(StratifiedKFold):\n",
    "    \"\"\"Stratified K-Folds cross-validator for continuous data.\n",
    "\n",
    "    Provides train/test indices to split data in train/test sets.\n",
    "\n",
    "    This cross-validation object is a variation of KFold that returns\n",
    "    stratified folds. The folds are made by preserving the percentage of\n",
    "    samples for each bin of the data.\n",
    "\n",
    "    \n",
    "    For visualisation of cross-validation behaviour and\n",
    "    comparison between common scikit-learn split methods\n",
    "    refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    \n",
    "    n_bins : int, default=10\n",
    "        Number of bins to make classes from the continuum. Must be at least 2.\n",
    "\n",
    "        .. versionchanged:: 0.22\n",
    "            ``n_splits`` default value changed from 3 to 5.\n",
    "\n",
    "    shuffle : bool, default=False\n",
    "        Whether to shuffle each class's samples before splitting into batches.\n",
    "        Note that the samples within each split will not be shuffled.\n",
    "\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        When `shuffle` is True, `random_state` affects the ordering of the\n",
    "        indices, which controls the randomness of each fold for each class.\n",
    "        Otherwise, leave `random_state` as `None`.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The implementation is designed to:\n",
    "\n",
    "    * Generate test sets such that all contain the same distribution of\n",
    "      bins, or as close as possible.\n",
    "    * Be invariant to bins label.\n",
    "    * Preserve order dependencies in the dataset ordering, when\n",
    "      ``shuffle=False``: all samples from class k in some test set were\n",
    "      contiguous in y, or separated in y by samples from classes other than k.\n",
    "    * Generate test sets where the smallest and largest differ by at most one\n",
    "      sample.\n",
    "\n",
    "    .. versionchanged:: 0.22\n",
    "        The previous implementation did not follow the last constraint.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, *, shuffle=False, random_state=None, n_bins=10):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def _make_test_folds(self, X, y=None):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        y = np.asarray(y)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "        allowed_target_types = (\"binary\", \"multiclass\", \"continuous\")\n",
    "        if type_of_target_y not in allowed_target_types:\n",
    "            raise ValueError(\n",
    "                \"Supported target types are: {}. Got {!r} instead.\".format(\n",
    "                    allowed_target_types, type_of_target_y\n",
    "                )\n",
    "            )\n",
    "        if type_of_target_y == \"continuous\":\n",
    "            y = KBinsDiscretizer(n_bins=10,\n",
    "                                 encode='ordinal',\n",
    "                                 strategy='uniform').fit_transform(y.reshape(-1, 1))\n",
    "            \n",
    "        y = column_or_1d(y)\n",
    "\n",
    "        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n",
    "        # y_inv encodes y according to lexicographic order. We invert y_idx to\n",
    "        # map the classes so that they are encoded by order of appearance:\n",
    "        # 0 represents the first label appearing in y, 1 the second, etc.\n",
    "        _, class_perm = np.unique(y_idx, return_inverse=True)\n",
    "        y_encoded = class_perm[y_inv]\n",
    "\n",
    "        n_classes = len(y_idx)\n",
    "        y_counts = np.bincount(y_encoded)\n",
    "        min_groups = np.min(y_counts)\n",
    "        if np.all(self.n_splits > y_counts):\n",
    "            raise ValueError(\n",
    "                \"n_splits=%d cannot be greater than the\"\n",
    "                \" number of members in each class.\" % (self.n_splits)\n",
    "            )\n",
    "        if self.n_splits > min_groups:\n",
    "            warnings.warn(\n",
    "                \"The least populated class in y has only %d\"\n",
    "                \" members, which is less than n_splits=%d.\"\n",
    "                % (min_groups, self.n_splits),\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "        # Determine the optimal number of samples from each class in each fold,\n",
    "        # using round robin over the sorted y. (This can be done direct from\n",
    "        # counts, but that code is unreadable.)\n",
    "        y_order = np.sort(y_encoded)\n",
    "        allocation = np.asarray(\n",
    "            [\n",
    "                np.bincount(y_order[i :: self.n_splits], minlength=n_classes)\n",
    "                for i in range(self.n_splits)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # To maintain the data order dependencies as best as possible within\n",
    "        # the stratification constraint, we assign samples from each class in\n",
    "        # blocks (and then mess that up when shuffle=True).\n",
    "        test_folds = np.empty(len(y), dtype=\"i\")\n",
    "        for k in range(n_classes):\n",
    "            # since the kth column of allocation stores the number of samples\n",
    "            # of class k in each test set, this generates blocks of fold\n",
    "            # indices corresponding to the allocation for class k.\n",
    "            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n",
    "            if self.shuffle:\n",
    "                rng.shuffle(folds_for_class)\n",
    "            test_folds[y_encoded == k] = folds_for_class\n",
    "        return test_folds\n",
    "\n",
    "    def _iter_test_masks(self, X, y=None, groups=None):\n",
    "        test_folds = self._make_test_folds(X, y)\n",
    "        for i in range(self.n_splits):\n",
    "            yield test_folds == i\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where `n_samples` is the number of samples\n",
    "            and `n_features` is the number of features.\n",
    "\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target variable for supervised learning problems.\n",
    "            Stratification is done based on the y labels.\n",
    "\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting `random_state`\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, input_name=\"y\", ensure_2d=False, dtype=None)\n",
    "        return super().split(X, y, groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
